{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional: Scikit-learn primer.\n",
    "In this additional assignment, you will learn to use the scikit-learn library. It is highly recommended to go through this notebook before starting with the final assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "All algorithms, both learning and pre-processing, in scikit-learn have been implemented with the same `fit`, `predict` and `transform` API. As soon as you have learned this API you can use any algorithm without having to implement it on your own. For a given learning problem, you can then apply all those algortihms in the same way. The API also hides all the complex optimization choices that have to be made. You can control these by changing the hyper-parameters. The effects of these choices have been well documented in the API documentation and the provided tutorials of scikit-learn.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, we will use the Iris dataset to keep things simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "X, y = load_iris(return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using classifiers\n",
    "Using a classifier in scikit-learn consist of 3 steps:\n",
    "1. Initialize the model. During this step, you can already give it some default hyper-parameters.\n",
    "2. Fitting the model on the training data.\n",
    "3. Making predictions and/or evaluating the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create\n",
    "Creating models is very easy in scikit-learn. All you have to do is create a new instance of the model's class.\n",
    "\n",
    "$ \\ex{1} $ Extent the list of models with the`SVC` and `LogisticRegression` algorithms. Give the SVM a `poly` kernel. Also, give both algorithms a regularization constant `C=0.5` and `random_state=42`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "random_state = 42\n",
    "\n",
    "models = {\n",
    "    \"GaussianNB\": GaussianNB(),\n",
    "    \"DummyClassifier\": DummyClassifier(strategy=\"most_frequent\"),\n",
    "    \"DecisionTreeClassifier\": DecisionTreeClassifier(max_depth=None, min_samples_leaf=2, random_state=random_state),\n",
    "    \"KNeighborsClassifier\": KNeighborsClassifier(n_neighbors=3, weights=\"distance\"),\n",
    "    # START ANSWER\n",
    "    \"SVM\": SVC(C=0.5, kernel=\"poly\", random_state=random_state),\n",
    "    \"LogisticRegression\": LogisticRegression(C=0.5, random_state=random_state),\n",
    "    # END ANSWER\n",
    "}\n",
    "\n",
    "assert \"GaussianNB\" in models and isinstance(models[\"GaussianNB\"], GaussianNB), \"There is no GaussianNB in models\"\n",
    "assert \"DecisionTreeClassifier\" in models and isinstance(models[\"DecisionTreeClassifier\"], DecisionTreeClassifier), \"There is no DecisionTreeClassifier in models\"\n",
    "assert \"KNeighborsClassifier\" in models and isinstance(models[\"KNeighborsClassifier\"], KNeighborsClassifier), \"There is no KNeighborsClassifier in models\"\n",
    "assert \"SVM\" in models and isinstance(models[\"SVM\"], SVC), \"There is no SVC in models\"\n",
    "assert \"LogisticRegression\" in models and isinstance(models[\"LogisticRegression\"], LogisticRegression), \"There is no LogisticRegression in models\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit\n",
    "$ \\ex{2} $ Fit each of your models on the entire training set by calling the `.fit` method of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, model in models.items():\n",
    "    # START ANSWER\n",
    "    models[name] = model.fit(X, y)\n",
    "    # END ANSWER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.validation import check_is_fitted\n",
    "\n",
    "for model in models.values():\n",
    "    check_is_fitted(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate\n",
    "The `sklearn.metrics` module has lots of metrics that can evaluate a model's predictions. Here is an example of how to calculate a model's F1 and accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB\n",
      "- accuracy_score 0.96\n",
      "- f1_score 0.96\n",
      "DummyClassifier\n",
      "- accuracy_score 0.3333333333333333\n",
      "- f1_score 0.5\n",
      "DecisionTreeClassifier\n",
      "- accuracy_score 0.98\n",
      "- f1_score 0.9800020002000202\n",
      "KNeighborsClassifier\n",
      "- accuracy_score 1.0\n",
      "- f1_score 1.0\n",
      "SVM\n",
      "- accuracy_score 0.9733333333333334\n",
      "- f1_score 0.973344004268374\n",
      "LogisticRegression\n",
      "- accuracy_score 0.9666666666666667\n",
      "- f1_score 0.9666700003333667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "for name, model in models.items():\n",
    "    prediction = model.predict(X)\n",
    "    f1_score_value = f1_score(prediction, y, average=\"weighted\")\n",
    "    accuracy = accuracy_score(prediction, y)\n",
    "    print(name)\n",
    "    print(\"- accuracy_score\", accuracy)\n",
    "    print(\"- f1_score\", f1_score_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data splitting\n",
    "Models usually achieve a high evaluation score on the training set. However, this doesn't say anything about how well it generalizes to unseen data. So we usually evaluate models using either a test/validation split or k-fold validation. Scikit-learn also makes our life easier here by implementing both functions for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test/validation split\n",
    "We can split datasets into training and test sets using the `train_test_split` function. The `test_size` parameter indicate the percentage of data that should go to the test set. The `stratify`  parameter indicate that the split should take the distribution of target labels `y` into account during the split. This parameter ensures that both the train and test have the same distribution of target variables.\n",
    "\n",
    "$ \\ex{3} $ The data has already been split into a training and a test set. Fit the model using the training set and evaluate them using the test set.\n",
    "\n",
    "The result on the test set should roughly be equal to:\n",
    "\n",
    "|                  Model |    F1 | Accuracy |\n",
    "|-----------------------:|------:|---------:|\n",
    "|             GaussianNB |  0.86 |     0.86 |\n",
    "| DummyClassifier        | 0.33  | 0.5      |\n",
    "| DecisionTreeClassifier | 0.866 |    0.866 |\n",
    "| KNeighborsClassifier   | 1     | 1        |\n",
    "| SVM                    | 0.93     | 0.934        |\n",
    "| LogisticRegression     | 0.933 | 0.934    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB   0.8666666666666667   0.8666666666666667\n",
      "DummyClassifier   0.16666666666666666   0.3333333333333333\n",
      "DecisionTreeClassifier   0.8666666666666667   0.8666666666666667\n",
      "KNeighborsClassifier   1.0   1.0\n",
      "SVM   0.9326599326599326   0.9333333333333333\n",
      "LogisticRegression   0.9326599326599326   0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42, shuffle=True, stratify=y)\n",
    "\n",
    "# START ANSWER \n",
    "for name, model in models.items():\n",
    "    models[name] = model.fit(X_train, y_train)\n",
    "    y_predict = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_predict)\n",
    "    F1 = f1_score(y_test, y_predict, average=\"weighted\")\n",
    "    print(name,' ',F1,' ',accuracy)\n",
    "# END ANSWER "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-fold validation\n",
    "Setting up k-fold validation is a bit more work but we can do it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "def k_fold_fit_and_evaluate(X, y, model, scoring_method, n_splits=5):\n",
    "    # define evaluation procedure\n",
    "    cv = KFold(n_splits=n_splits, random_state=42, shuffle=True)\n",
    "    # evaluate model\n",
    "    scores = cross_validate(model, X, y, scoring=scoring_method, cv=cv, n_jobs=-1)\n",
    "    \n",
    "       \n",
    "    return scores[\"test_score\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: `cross_validate` expects a `scoring_method`. We can create a `scoring_method` using the `make_scorer` function from scikit-learn.\n",
    "\n",
    "$ \\ex{4} $ Use the example below to calculate the mean and std for both the F1 and the accuracy score. The `k_fold_fit_and_evaluate` method returns the resulting k-fold validation score from the provided `scoring_method`.\n",
    "\n",
    "Hint: use `np.mean` and `np.std`.\n",
    "\n",
    "The result using k-fold validation should roughly be equal to:\n",
    "\n",
    "\n",
    "|                  Model | mean F1 | std F1 | mean Accuracy | std Accuracy |\n",
    "|-----------------------:|--------:|--------|--------------:|--------------|\n",
    "|             GaussianNB |   0.959 | 0.0249 |        0.960 | 0.0249        |\n",
    "| DummyClassifier        | 0.107   | 0.0187 | 0.26        | 0.0249       |\n",
    "| DecisionTreeClassifier |   0.946 | 0.0338 |       0.94655 | 0.0338       |\n",
    "| KNeighborsClassifier   | 0.966   | 0.0214 | 0.9663        | 0.02144      |\n",
    "| SVM                    | 0.980   | 0.0163 | 0.980        | 0.0163      |\n",
    "| LogisticRegression     | 0.966   | 0.0298 | 0.966        | 0.0298     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB\n",
      " mean F1: \t  std F1\n",
      "0.959916 \t 0.024923\n",
      " mean Accuracy \t  std Accuracy\n",
      "0.960000 \t 0.024944\n",
      "\n",
      "DummyClassifier\n",
      " mean F1: \t  std F1\n",
      "0.107920 \t 0.018663\n",
      " mean Accuracy \t  std Accuracy\n",
      "0.260000 \t 0.024944\n",
      "\n",
      "DecisionTreeClassifier\n",
      " mean F1: \t  std F1\n",
      "0.946560 \t 0.033900\n",
      " mean Accuracy \t  std Accuracy\n",
      "0.946667 \t 0.033993\n",
      "\n",
      "KNeighborsClassifier\n",
      " mean F1: \t  std F1\n",
      "0.966362 \t 0.021444\n",
      " mean Accuracy \t  std Accuracy\n",
      "0.966667 \t 0.021082\n",
      "\n",
      "SVM\n",
      " mean F1: \t  std F1\n",
      "0.979979 \t 0.016348\n",
      " mean Accuracy \t  std Accuracy\n",
      "0.980000 \t 0.016330\n",
      "\n",
      "LogisticRegression\n",
      " mean F1: \t  std F1\n",
      "0.966667 \t 0.029814\n",
      " mean Accuracy \t  std Accuracy\n",
      "0.966667 \t 0.029814\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "import numpy as np\n",
    "\n",
    "n_splits = 5\n",
    "\n",
    "\n",
    "scoring_method_F1 = make_scorer(lambda prediction, true_target: f1_score(prediction, true_target, average=\"weighted\"))\n",
    "# START ANSWER \n",
    "scoring_method_acc = make_scorer(lambda prediction, true_target: accuracy_score(prediction, true_target))\n",
    "# END ANSWER\n",
    "\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(name)\n",
    "    metrics_F1 = k_fold_fit_and_evaluate(X, y, model, scoring_method_F1, n_splits=n_splits)\n",
    "    # START ANSWER\n",
    "    metrics_acc = k_fold_fit_and_evaluate(X, y, model, scoring_method_acc, n_splits=n_splits)\n",
    "    print(\" mean F1:\", '\\t', \" std F1\")\n",
    "    print(format(np.mean(metrics_F1),\"5f\"), '\\t', format(np.std(metrics_F1),\"5f\"))\n",
    "    print(\" mean Accuracy\", '\\t', \" std Accuracy\")\n",
    "    print(format(np.mean(metrics_acc),\"5f\"), '\\t', format(np.std(metrics_acc),\"5f\"))\n",
    "    print()\n",
    "    # END ANSWER "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search\n",
    "Scikit-learn also makes it easier to tune <span class=\"mark\">hyper-parameters</span> using `GridSearchCV`.\n",
    "\n",
    "$ \\ex{5} $ Extend the `model_parameters` dict by specifying a grid search for the `KNeighborsClassifier`, <span class=\"mark\">`SVM`</span> and the `LogisticRegression` models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB\n",
      "- best_score = 0.9599161225248183\n",
      "best paramters:\n",
      "DummyClassifier\n",
      "- best_score = 0.10791990370937739\n",
      "best paramters:\n",
      "DecisionTreeClassifier\n",
      "- best_score = 0.9465598893859765\n",
      "best paramters:\n",
      "- max_depth None\n",
      "- random_state 42\n",
      "KNeighborsClassifier\n",
      "- best_score = 0.9797720797720798\n",
      "best paramters:\n",
      "- n_neighbors 7\n",
      "- weights distance\n",
      "SVM\n",
      "- best_score = 0.9799785345717235\n",
      "best paramters:\n",
      "- C 0.2\n",
      "- random_state 42\n",
      "LogisticRegression\n",
      "- best_score = 0.9732912280701754\n",
      "best paramters:\n",
      "- C 0.7\n",
      "- random_state 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programmer\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "random_state = 42\n",
    "n_splits = 5\n",
    "scoring_method = make_scorer(lambda prediction, true_target: f1_score(prediction, true_target, average=\"weighted\"))\n",
    "\n",
    "model_parameters = {\n",
    "    \"GaussianNB\": {\n",
    "    \n",
    "    },\n",
    "    \"DummyClassifier\": {\n",
    "        \n",
    "    },\n",
    "    \"DecisionTreeClassifier\": {\n",
    "        'random_state': [random_state],\n",
    "        'max_depth': [None, 2, 5, 10]\n",
    "    },\n",
    "    # START ANSWER\n",
    "    \"KNeighborsClassifier\": {\n",
    "        \"n_neighbors\": [i for i in range(1,11)],\n",
    "        \"weights\": [\"uniform\", \"distance\"] \n",
    "    },\n",
    "    \"SVM\": {\n",
    "        \"C\": [i/10 for i in range(1,11)],\n",
    "#         \"kernal\": [\"linear\", \"poly\", \"rbf\", \"sigmoid\", \"precomputed\"],\n",
    "        \"random_state\": [random_state]\n",
    "    },\n",
    "    \"LogisticRegression\": {\n",
    "        \"C\": [i/10 for i in range(1,11)],\n",
    "        \"random_state\": [random_state]\n",
    "    }\n",
    "    # END ANSWER\n",
    "}\n",
    "\n",
    "for model_name, parameters in model_parameters.items():\n",
    "    model = models[model_name]\n",
    "    \n",
    "    cv = KFold(n_splits=n_splits, random_state=random_state, shuffle=True)\n",
    "    grid_search = GridSearchCV(model, parameters, cv=cv, n_jobs=-1, verbose=False, scoring=scoring_method).fit(X, y)\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_score = grid_search.best_score_\n",
    "    best_params = grid_search.best_params_\n",
    "    \n",
    "    print(model_name)\n",
    "    print(\"- best_score =\", best_score)\n",
    "    print(\"best paramters:\")\n",
    "    for k,v in best_params.items():\n",
    "        print(\"-\", k, v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Transformers\n",
    "The transformers have a similar but slightly different API than the models. Transformers still have the `fit` method. The fit method is, for example, use in the `StandardScaler` to find the `mean` and `std` values. However, the `predict` method is replaced with the `transform` method. Scikit-learn did this to make it clear to the users that this is not a model but a feature transformer.\n",
    "\n",
    "\n",
    "<span class=\"mark\">Q: Why transform? Which method needs transformation? The transforming objects?</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([5.82962963, 3.05703704, 3.75111111, 1.20518519]),\n",
       " array([0.82210877, 0.44297659, 1.74999965, 0.763842  ]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "# 概率模型（树形模型）不需要归一化,\n",
    "# 因为它们不关心变量的值，而是关心变量的分布和变量之间的条件概率，\n",
    "# 如决策树、RF。而像Adaboost、SVM、LR、Knn、KMeans之类的最优化问题就需要归一化。\n",
    "scaler.mean_, scaler.scale_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After fitting the transformer, you can call the `transform` method, and it will transform the input features based on the parameters it found during the last `fit` call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train\n",
      "mean 3.460740740740741\n",
      "std 1.9662465199534571\n",
      "\n",
      "X_train_transformed\n",
      "mean 6.579099405186112e-17\n",
      "std 1.0\n"
     ]
    }
   ],
   "source": [
    "X_train_transformed = scaler.transform(X_train)\n",
    "print(\"X_train\")\n",
    "print(\"mean\", X_train.mean())\n",
    "print(\"std\", X_train.std())\n",
    "print()\n",
    "print(\"X_train_transformed\")\n",
    "print(\"mean\", X_train_transformed.mean())\n",
    "print(\"std\", X_train_transformed.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\ex{6} $ First, transform the dataset using the `Normalizer` transformer. The fit and evaluate each model using the transformed features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train\n",
      "mean 3.460740740740741\n",
      "std 1.9662465199534571\n",
      "\n",
      "X_train_Normalized\n",
      "mean 0.43844230430986214\n",
      "std 0.24035046451267425\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42, shuffle=True, stratify=y)\n",
    "\n",
    "scaler = preprocessing.Normalizer().fit(X_train)\n",
    "\n",
    "# START ANSWER\n",
    "X_train_Normalized = scaler.transform(X_train)\n",
    "print(\"X_train\")\n",
    "print(\"mean\", X_train.mean())\n",
    "print(\"std\", X_train.std())\n",
    "print()\n",
    "print(\"X_train_Normalized\")\n",
    "print(\"mean\", X_train_Normalized.mean())\n",
    "print(\"std\", X_train_Normalized.std())\n",
    "print()\n",
    "\n",
    "# for name, model in models.items():\n",
    "#     model_original = model.fit(X_train, y_train)\n",
    "#     y_predict = model_original.predict(X_test)\n",
    "#     F1 = f1_score(y_test, y_predict, average=\"weighted\")\n",
    "#     print(name,' ',F1)\n",
    "    \n",
    "#     model_nomalized = model.fit(X_train_Normalized, y_train)\n",
    "#     y_predict_nomalized = model_nomalized.predict(X_test)\n",
    "#     F1 = f1_score(y_test, y_predict_nomalized, average=\"weighted\")\n",
    "#     print(\"Nomalized \" + str(name),' ',F1)\n",
    "#     print()\n",
    "# END ANSWER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB   0.8666666666666667\n",
      "Nomalized GaussianNB   1.0\n",
      "Standardlized GaussianNB   0.8666666666666667\n",
      "\n",
      "DummyClassifier   0.16666666666666666\n",
      "Nomalized DummyClassifier   0.16666666666666666\n",
      "Standardlized DummyClassifier   0.16666666666666666\n",
      "\n",
      "DecisionTreeClassifier   0.8666666666666667\n",
      "Nomalized DecisionTreeClassifier   0.7802197802197803\n",
      "Standardlized DecisionTreeClassifier   0.8666666666666667\n",
      "\n",
      "KNeighborsClassifier   1.0\n",
      "Nomalized KNeighborsClassifier   1.0\n",
      "Standardlized KNeighborsClassifier   0.9326599326599326\n",
      "\n",
      "SVM   0.9326599326599326\n",
      "Nomalized SVM   1.0\n",
      "Standardlized SVM   0.861111111111111\n",
      "\n",
      "LogisticRegression   0.9326599326599326\n",
      "Nomalized LogisticRegression   0.861111111111111\n",
      "Standardlized LogisticRegression   0.8666666666666667\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "scaler_norm = preprocessing.Normalizer().fit(X,y)\n",
    "scaler_stand = preprocessing.StandardScaler().fit(X,y)\n",
    "X_norm = scaler_norm.transform(X)\n",
    "X_stand = scaler_stand.transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.1, \n",
    "                                                    random_state=42, \n",
    "                                                    shuffle=True, \n",
    "                                                    stratify=y\n",
    "                                                   )\n",
    "X_train_norm, X_test_norm, y_train, y_test = train_test_split(X_norm, y, \n",
    "                                                              test_size=0.1, \n",
    "                                                              random_state=42, \n",
    "                                                              shuffle=True, \n",
    "                                                              stratify=y\n",
    "                                                             )\n",
    "X_train_stand, X_test_stand, y_train, y_test = train_test_split(X_stand, y, \n",
    "                                                              test_size=0.1, \n",
    "                                                              random_state=42, \n",
    "                                                              shuffle=True, \n",
    "                                                              stratify=y\n",
    "                                                             )\n",
    "\n",
    "for name, model in models.items():\n",
    "    model_original = model.fit(X_train, y_train)\n",
    "    y_predict = model_original.predict(X_test)\n",
    "    F1 = f1_score(y_test, y_predict, average=\"weighted\")\n",
    "    print(name,' ',F1)\n",
    "    \n",
    "    model_nomalized = model.fit(X_train_norm, y_train)\n",
    "    y_predict_nomalized = model_nomalized.predict(X_test_norm)\n",
    "    F1 = f1_score(y_test, y_predict_nomalized, average=\"weighted\")\n",
    "    print(\"Nomalized \" + str(name),' ',F1)\n",
    "    \n",
    "    model_standerlized = model.fit(X_train_stand, y_train)\n",
    "    y_predict_standerlized = model_nomalized.predict(X_test_stand)\n",
    "    F1 = f1_score(y_test, y_predict_standerlized, average=\"weighted\")\n",
    "    print(\"Standardlized \" + str(name),' ',F1)\n",
    "    print()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
